[
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Updated navigation menu to follow Poldrack Lab structure\nAdded “About” dropdown menu with Values, Labs, and Collaborations\nReorganized navigation: Research → Projects → Publications → People → Teaching → Resources → About → Join Us → Contact\nEnhanced home page with section overview and clear calls-to-action\nAdded breadcrumb navigation to all main pages\nImproved page formatting and structure for all main sections\n\n\n\n\n\n\n\nProjects page: Add detailed descriptions for Behaverse and CERISE projects\nPublications page: Implement automated Google Scholar integration\nTeaching page: Add detailed course descriptions and learning objectives\nResources page: Add actual links to GitHub repositories and datasets\nCollaborations page: Add affiliations and collaboration details for all partners\n\n\n\n\n\nFix empty href in line 35: “Learn more about the PhD journey at the University of Luxembourg”\nFix empty href in line 45: “Learn more about this position” (PhD in AI and Cognitive Sciences)\nFix empty href in line 56: “Learn more about this position” (PhD in Cybersecurity)\nFix empty href in line 94: “Learn more about Jobs available at UniLu”\n\n\n\n\nIn research.qmd: - [ ] Line 25: Fix “cognitve assessment” → “cognitive assessment” - [ ] Line 25: Fix “cognitve training” → “cognitive training”\nIn people/pi/cardosoleite_pedro.qmd: - [ ] Line 46: Fix “pyschologist” → “psychologist” - [ ] Line 48: Fix “impac” → “impact” - [ ] Line 48: Fix “suporting” → “supporting” - [ ] Line 52: Fix “partiuclar” → “particular” - [ ] Line 52: Fix “Univerisy” → “University” - [ ] Line 52: Fix “reasearch” → “research”\nIn join_us.qmd: - [ ] Line 69: Fix “Univeristy” → “University”\n\n\n\nIn research.qmd: - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive assessment - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive training - [ ] Line 39: Verify and fix “behaverse.org” link\n\n\n\n\n\n\nReplace placeholder.png with actual photos for: - [ ] Hoorieh Afkari (people/members/afkari_hoorieh.qmd) - [ ] Sophie Doublet (people/members/doublet_sophie.qmd) - [ ] Suvadeep Mukherjee (people/members/mukherjee_suvadeep.qmd) - [ ] Hainan Yu (people/members/yu_hainan.qmd) - [ ] Ziming Wang (people/phd_candidates/wang_ziming.qmd) - [ ] Dominic Mussac (people/past_members/mussac_dominic.qmd) - [ ] Emmanuel Schmuck (people/past_members/schmuck_emmanuel.qmd)\n\n\n\n\nHome page: Add lab photo or showcase images\nResearch page: Complete fragmented sentences and improve flow\nProjects page: Add project timelines, funding information, and outcomes\nTeaching page: Add course schedules, prerequisites, and enrollment information\nValues page: Consider adding a formal lab manual/guide\nLabs page: Add photos of facilities and equipment\n\n\n\n\n\nCheck and update all team member status and roles for 2025\nStandardize bio formats across all team members\nVerify all external links in team member profiles work correctly\nAdd research interests and key publications to each profile\n\n\n\n\n\n\n\n\nConsider implementing design elements inspired by Anthropic/IDEO websites\nAdd visual hierarchy improvements\nImplement better grid layouts for content sections\nConsider custom CSS for better visual appeal\nAdd hover effects and interactive elements\n\n\n\n\n\nAdd meta descriptions for better SEO\nAdd keywords for search optimization\nReview color contrast for accessibility compliance\nAdd alt text for all images\nOptimize image sizes and formats for better performance\n\n\n\n\n\nAdd search functionality\nConsider adding a news/blog section for lab updates\nAdd contact form instead of just email link\nImplement smooth scrolling and page transitions\nAdd “back to top” buttons on long pages\n\n\n\n\n\nGet professional headshots for all team members\nAdd testimonials or impact stories\nCreate infographics for research methodology\nAdd lab achievements and awards section\nConsider adding timeline of lab milestones\n\n\n\n\n\n\n\n\nImplement automated publication fetching from Google Scholar\nAdd interactive research visualizations\nCreate online lab tour or virtual reality experience\nAdd multilingual support (English/French/German)\nImplement content management system for easier updates\n\n\n\n\n\nSocial media integration\nGoogle Analytics tracking verification\nNewsletter signup integration\nEvent calendar integration\nOnline application system for positions\n\n\n\n\n\n\nTest all external links\nVerify all internal navigation works correctly\nCheck website responsiveness on mobile devices\nValidate HTML markup\nTest website loading speed\nEnsure all images load correctly\nVerify contact email works correctly\nTest new navigation structure across all devices\n\n\nNote: The website structure has been significantly improved to match the Poldrack Lab organization. Priority should now focus on content development and fixing remaining technical issues.\nLast Updated: September 3, 2025"
  },
  {
    "objectID": "TODO.html#completed-website-structure-reorganization",
    "href": "TODO.html#completed-website-structure-reorganization",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Updated navigation menu to follow Poldrack Lab structure\nAdded “About” dropdown menu with Values, Labs, and Collaborations\nReorganized navigation: Research → Projects → Publications → People → Teaching → Resources → About → Join Us → Contact\nEnhanced home page with section overview and clear calls-to-action\nAdded breadcrumb navigation to all main pages\nImproved page formatting and structure for all main sections"
  },
  {
    "objectID": "TODO.html#high-priority-issues-critical-fixes",
    "href": "TODO.html#high-priority-issues-critical-fixes",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Projects page: Add detailed descriptions for Behaverse and CERISE projects\nPublications page: Implement automated Google Scholar integration\nTeaching page: Add detailed course descriptions and learning objectives\nResources page: Add actual links to GitHub repositories and datasets\nCollaborations page: Add affiliations and collaboration details for all partners\n\n\n\n\n\nFix empty href in line 35: “Learn more about the PhD journey at the University of Luxembourg”\nFix empty href in line 45: “Learn more about this position” (PhD in AI and Cognitive Sciences)\nFix empty href in line 56: “Learn more about this position” (PhD in Cybersecurity)\nFix empty href in line 94: “Learn more about Jobs available at UniLu”\n\n\n\n\nIn research.qmd: - [ ] Line 25: Fix “cognitve assessment” → “cognitive assessment” - [ ] Line 25: Fix “cognitve training” → “cognitive training”\nIn people/pi/cardosoleite_pedro.qmd: - [ ] Line 46: Fix “pyschologist” → “psychologist” - [ ] Line 48: Fix “impac” → “impact” - [ ] Line 48: Fix “suporting” → “supporting” - [ ] Line 52: Fix “partiuclar” → “particular” - [ ] Line 52: Fix “Univerisy” → “University” - [ ] Line 52: Fix “reasearch” → “research”\nIn join_us.qmd: - [ ] Line 69: Fix “Univeristy” → “University”\n\n\n\nIn research.qmd: - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive assessment - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive training - [ ] Line 39: Verify and fix “behaverse.org” link"
  },
  {
    "objectID": "TODO.html#medium-priority-issues",
    "href": "TODO.html#medium-priority-issues",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Replace placeholder.png with actual photos for: - [ ] Hoorieh Afkari (people/members/afkari_hoorieh.qmd) - [ ] Sophie Doublet (people/members/doublet_sophie.qmd) - [ ] Suvadeep Mukherjee (people/members/mukherjee_suvadeep.qmd) - [ ] Hainan Yu (people/members/yu_hainan.qmd) - [ ] Ziming Wang (people/phd_candidates/wang_ziming.qmd) - [ ] Dominic Mussac (people/past_members/mussac_dominic.qmd) - [ ] Emmanuel Schmuck (people/past_members/schmuck_emmanuel.qmd)\n\n\n\n\nHome page: Add lab photo or showcase images\nResearch page: Complete fragmented sentences and improve flow\nProjects page: Add project timelines, funding information, and outcomes\nTeaching page: Add course schedules, prerequisites, and enrollment information\nValues page: Consider adding a formal lab manual/guide\nLabs page: Add photos of facilities and equipment\n\n\n\n\n\nCheck and update all team member status and roles for 2025\nStandardize bio formats across all team members\nVerify all external links in team member profiles work correctly\nAdd research interests and key publications to each profile"
  },
  {
    "objectID": "TODO.html#low-priority-improvements",
    "href": "TODO.html#low-priority-improvements",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Consider implementing design elements inspired by Anthropic/IDEO websites\nAdd visual hierarchy improvements\nImplement better grid layouts for content sections\nConsider custom CSS for better visual appeal\nAdd hover effects and interactive elements\n\n\n\n\n\nAdd meta descriptions for better SEO\nAdd keywords for search optimization\nReview color contrast for accessibility compliance\nAdd alt text for all images\nOptimize image sizes and formats for better performance\n\n\n\n\n\nAdd search functionality\nConsider adding a news/blog section for lab updates\nAdd contact form instead of just email link\nImplement smooth scrolling and page transitions\nAdd “back to top” buttons on long pages\n\n\n\n\n\nGet professional headshots for all team members\nAdd testimonials or impact stories\nCreate infographics for research methodology\nAdd lab achievements and awards section\nConsider adding timeline of lab milestones"
  },
  {
    "objectID": "TODO.html#future-enhancements",
    "href": "TODO.html#future-enhancements",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Implement automated publication fetching from Google Scholar\nAdd interactive research visualizations\nCreate online lab tour or virtual reality experience\nAdd multilingual support (English/French/German)\nImplement content management system for easier updates\n\n\n\n\n\nSocial media integration\nGoogle Analytics tracking verification\nNewsletter signup integration\nEvent calendar integration\nOnline application system for positions"
  },
  {
    "objectID": "TODO.html#verification-tasks",
    "href": "TODO.html#verification-tasks",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Test all external links\nVerify all internal navigation works correctly\nCheck website responsiveness on mobile devices\nValidate HTML markup\nTest website loading speed\nEnsure all images load correctly\nVerify contact email works correctly\nTest new navigation structure across all devices\n\n\nNote: The website structure has been significantly improved to match the Poldrack Lab organization. Priority should now focus on content development and fixing remaining technical issues.\nLast Updated: September 3, 2025"
  },
  {
    "objectID": "scripts/publications.html",
    "href": "scripts/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\nUnknown Year\n\nUnknown Authors (Unknown Year). SUR LA RELATION PERCEPTION-ACTION: TEMPS DE REACTION ET JUGEMENT D’ORDRE TEMPOREL. Unknown Journal\n\n\n\n2025\n\nUnknown Authors (2025). Prototyping Video Games for Understanding Abstract Concepts. Unknown Journal\nUnknown Authors (2025). Clinical and cognitive assessment in Friedreich ataxia clinical trials: a review. Unknown Journal\nUnknown Authors (2025). YARE-GAN: Yet Another Resting State EEG-GAN. Unknown Journal\nUnknown Authors (2025). Sampling Rate and Task Selection in EEG-Authentication. Unknown Journal\n\n\n\n2024\n\nUnknown Authors (2024). Balancing the perception of cheating detection, privacy and fairness: A mixed-methods study of visual data obfuscation in remote proctoring. Unknown Journal\nUnknown Authors (2024). AI4T-Comparative European Evaluation Report. Unknown Journal\nUnknown Authors (2024). Analysis of coordination mechanisms during collaborative problem-solving on an interactive tabletop display. Unknown Journal\n\n\n\n2023\n\nUnknown Authors (2023). Video games to study and improve collaboration skills. Unknown Journal\nUnknown Authors (2023). The impact of cognitive characteristics and image-based semantic embeddings on item difficulty. Unknown Journal\nUnknown Authors (2023). AI4T National Evaluation Report-France. Unknown Journal\nUnknown Authors (2023). CogPonder: Towards a Computational Framework of General Cognitive Control. Unknown Journal\nUnknown Authors (2023). Temporary self-deprivation can impair cognitive control: Evidence from the Ramadan fast. Unknown Journal\nUnknown Authors (2023). Towards a Computational Model of General Cognitive Control Using Artificial Intelligence, Experimental Psychology and Cognitive Neuroscience. Unknown Journal\nUnknown Authors (2023). Decoding Hypnotic Experience from Raw EEG using a Multi-Output Auto-Encoder. Unknown Journal\nUnknown Authors (2023). Establishing awareness through pointing gestures during collaborative decision-making in a wall-display environment. Unknown Journal\nUnknown Authors (2023). Seize the moment: the role of scrub nurses’ proactivity in microsurgical operating-room collaborations. Unknown Journal\nUnknown Authors (2023). ‘How Do We Move Back?’–A Case Study of Joint Problem-Solving at an Interactive Tabletop Mediated Activity. Unknown Journal\n\n\n\n2022\n\nUnknown Authors (2022). Linking theories and methods in cognitive sciences via joint embedding of the scientific literature: The example of cognitive control. Unknown Journal\nUnknown Authors (2022). Cogenv: A reinforcement learning environment for cognitive tests. Unknown Journal\nUnknown Authors (2022). Validierung und psychometrische Analyse automatisch generierter Mathematikaufgaben im Bereich Zahlen und Operationen. Unknown Journal\nUnknown Authors (2022). Validation and Psychometric Analysis of 32 cognitive item models spanning Grades 1 to 7 in the mathematical domain of numbers & operations. Unknown Journal\nUnknown Authors (2022). Optimal learning under structural environmental uncertainty reveals inherent learning trade-offs. Unknown Journal\nUnknown Authors (2022). CogEnv: A Reinforcement Learning Environment for Cognitive Tests. Unknown Journal\nUnknown Authors (2022). A machine translation-powered chatbot for public administration. Unknown Journal\nUnknown Authors (2022). ENRICH4ALL: A first Luxembourgish BERT Model for a Multilingual Chatbot. Unknown Journal\nUnknown Authors (2022). Deborah: a web-based cross-device orchestration layer. Unknown Journal\n\n\n\n2021\n\nUnknown Authors (2021). Media use, attention, mental health and academic performance among 8 to 12 year old children. Unknown Journal\nUnknown Authors (2021). A mixture of generative models strategy helps humans generalize across tasks. Unknown Journal\nUnknown Authors (2021). Training cognition with video games. Unknown Journal\nUnknown Authors (2021). A Mixture of Generative Models Strategy Helps Humans Generalize across Tasks. Unknown Journal\nUnknown Authors (2021). Digitalisation du diagnostic pédagogique: De l’évolution à la révolution. Unknown Journal\nUnknown Authors (2021). Digitalisierung der pädagogischen Diagnostik: Von Evolution zu Revolution. Unknown Journal\nUnknown Authors (2021). Ramadan, Risk-taking, and Cooperation in Resource Dilemmas: Communal Deprivation as a Cultural Response to Scarcity?. Unknown Journal\nUnknown Authors (2021). Balancing Shareability and Positive Interdependence to Support Collaborative Problem‐Solving on Interactive Tabletops. Unknown Journal\nUnknown Authors (2021). Networked apparatus, system and method for monitoring transient occupancy. Unknown Journal\nUnknown Authors (2021). Research Article Balancing Shareability and Positive Interdependence to Support Collaborative Problem-Solving on Interactive Tabletops. Unknown Journal\nUnknown Authors (2021). Orbitia–Gemeinsam auf Mission.. Unknown Journal\n\n\n\n2020\n\nUnknown Authors (2020). 18 Games for Enhancing Cognitive Abilities. Unknown Journal\nUnknown Authors (2020). Contrasting classical and machine learning approaches in the estimation of value-added scores in large-scale educational data. Unknown Journal\nUnknown Authors (2020). The structure of behavioral data. Unknown Journal\nUnknown Authors (2020). Tackling educational inequalities using school effectiveness measures. Unknown Journal\nUnknown Authors (2020). Can machine learning methods lead to more precise measures of school effectiveness? An application of various machine learning approaches in the estimation of school value …. Unknown Journal\nUnknown Authors (2020). Developing an interactive tabletop mediated activity to induce collaboration by implementing design considerations based on cooperative learning principles. Unknown Journal\nUnknown Authors (2020). “You move THIS!”: Annotation of Pointing Gestures on Tabletop Interfaces in Low Awareness Situations. Unknown Journal\nUnknown Authors (2020). Designing different features of an interactive tabletop application to support collaborative problem-solving. Unknown Journal\nUnknown Authors (2020). ‘Being a space mining crew’: How participants jointly discover their complementary resources while engaging into a serious game at an interactive tabletop. Unknown Journal\nUnknown Authors (2020). Exploring Opportunities of Tabletop Interfaces for Promoting and Analysing Collaboration.. Unknown Journal\n\n\n\n2019\n\nUnknown Authors (2019). Principles underlying the design of a cognitive training game as a research framework. Unknown Journal\nUnknown Authors (2019). Towards discovering problem similarity through deep learning: combining problem features and user behavior.. Unknown Journal\nUnknown Authors (2019). A Formal Framework for Structured N-Back Stimuli Sequences. Unknown Journal\nUnknown Authors (2019). A Multi-Objective Optimization Algorithm to Generate Unbiased Stimuli Sequences for Cognitive Tasks. Unknown Journal\nUnknown Authors (2019). A generalizable performance evaluation model of driving games via risk-weighted trajectories. Unknown Journal\nUnknown Authors (2019). Designing collaborative scenarios on tangible tabletop interfaces-insights from the implementation of paper prototypes in the context of a multidisciplinary design workshop. Unknown Journal\nUnknown Authors (2019). Active tangibles for tabletop interaction based on the Kniwwelino prototyping platform. Unknown Journal\n\n\n\n2018\n\nUnknown Authors (2018). Interaction in the micro-surgery operating room: the potentials for gaze-based interaction with the surgical microscope. Unknown Journal\nUnknown Authors (2018). Command Selection in Gaze-based See-through Virtual Image-Guided Environments. Unknown Journal\n\n\n\n2017\n\nUnknown Authors (2017). The impact of action video game training on mathematical abilities in adults. Unknown Journal\nUnknown Authors (2017). Prematurity Stereotyping: Absence of Evidence (DRAFT 1. DEC2015). Unknown Journal\nUnknown Authors (2017). Optimal eye movement strategies: a comparison of neurosurgeons gaze patterns when using a surgical microscope. Unknown Journal\nUnknown Authors (2017). Towards automatic skill evaluation in microsurgery. Unknown Journal\n\n\n\n2016\n\nUnknown Authors (2016). Technology consumption and cognitive control: Contrasting action video game experience with media multitasking. Unknown Journal\nUnknown Authors (2016). Mechanisms for maintaining situation awareness in the micro-neurosurgical operating room. Unknown Journal\n\n\n\n2015\n\nUnknown Authors (2015). On the impact of new technologies on multitasking. Unknown Journal\nUnknown Authors (2015). Methods to test visual attention online. Unknown Journal\nUnknown Authors (2015). Analysis of disruptive events and precarious situations caused by interaction with neurosurgical microscope. Unknown Journal\n\n\n\n2014\n\nUnknown Authors (2014). Video game play, attention, and learning: how to shape the development of attention and influence learning?. Unknown Journal\nUnknown Authors (2014). Human perceptual decision making: Disentangling task onset and stimulus onset. Unknown Journal\nUnknown Authors (2014). Summation versus suppression in metacontrast masking: On the potential pitfalls of using metacontrast masking to assess perceptual–motor dissociation. Unknown Journal\nUnknown Authors (2014). The potentials for hands-free interaction in micro-neurosurgery. Unknown Journal\n\n\n\n2013\n\nUnknown Authors (2013). Predicting network attacks using ontology-driven inference. Unknown Journal\n\n\n\n2012\n\nUnknown Authors (2012). Action effect anticipation: neurophysiological basis and functional consequences. Unknown Journal\nUnknown Authors (2012). Brain plasticity: Paradoxical case of a neurodegenerative disease?. Unknown Journal\nUnknown Authors (2012). Ontology-based modeling of DDoS attacks for attack plan detection. Unknown Journal\n\n\n\n2010\n\nUnknown Authors (2010). A new look at sensory attenuation: Action-effect anticipation affects sensitivity, not response bias. Unknown Journal\nUnknown Authors (2010). On the perceptual/motor dissociation: a review of concepts, theory, experimental paradigms and data interpretations. Unknown Journal\nUnknown Authors (2010). Introspective duration estimation of reactive and proactive motor responses. Unknown Journal\nUnknown Authors (2010). On the perceptual/motor dissociation: A review of concepts, theory, experimental paradigms and data interpretations. Seeing and Perceiving, 23 (2), 89–151. Unknown Journal\nUnknown Authors (2010). Influence of near threshold visual distractors on perceptual detection and reaching movements. Unknown Journal\nUnknown Authors (2010). Articles in PresS. J Neurophysiol (August 11, 2010). doi: 10.1152/jn. 01123.2009. Unknown Journal\n\n\n\n2009\n\nUnknown Authors (2009). Comparison of perceptual and motor latencies via anticipatory and reactive response times. Unknown Journal\nUnknown Authors (2009). Comparison of perceptual and motor decisions via confidence judgments and saccade curvature. Unknown Journal\nUnknown Authors (2009). The effect of spatial distractors on visuomotor responses depends on their detection: evidence for no dissociation between perception and action. Unknown Journal\nUnknown Authors (2009). A Fault Injection Attitude based on Background Debug Mode in Embedded Systems. Unknown Journal\nUnknown Authors (2009). An innovative fault injection method in embedded systems via background debug mode. Unknown Journal\nUnknown Authors (2009). Anytime Operating System. Unknown Journal\nUnknown Authors (2009). Serial Ports in Interfacing Circuits. Unknown Journal\nUnknown Authors (2009). ارائه يک مکانيزم جهت پورت کردن سيستم­ عامل­‌های بی‌درنگ برای کاربردهای سيستم­‌های نهفته‎. Unknown Journal\n\n\n\n2008\n\nUnknown Authors (2008). Comparaison des réponses perceptives et motrices dans les tâches psychophysiques élémentaires. Unknown Journal\nUnknown Authors (2008). A negative test of the sensorimotor dissociation with saccades perturbed by close to threshold distractors. Unknown Journal\n\n\n\n2007\n\nUnknown Authors (2007). Temporal order judgment and simple reaction times: Evidence for a common processing system. Unknown Journal\nUnknown Authors (2007). Perceptual criterion and motor threshold: a signal detection analysis of the relationship between perception and action. Unknown Journal\nUnknown Authors (2007). Anticipatory vs. reactive response times: A new method to compare perceptual and motor latencies. Unknown Journal\nUnknown Authors (2007). The Perceptual-motor dissociation tested negatively with a standard 2AFC task. Unknown Journal\nUnknown Authors (2007). Temporal order judgment and simple reaction times. Unknown Journal\nUnknown Authors (2007). Evidence against an active filling-in process through the blind spot. Unknown Journal\nUnknown Authors (2007). New evidence against a perceptual-motor dissociation. Unknown Journal\n\n\n\n2006\n\nUnknown Authors (2006). A negative test of the sensorimotor dissociation via a trial-by-trial analysis of response times and temporal order judgments. Unknown Journal\nUnknown Authors (2006). SOS 2006: Team Description Paper. Unknown Journal\n\n\n\n2005\n\nUnknown Authors (2005). Time perception of near-threshold visual events. Unknown Journal"
  },
  {
    "objectID": "join_us.html",
    "href": "join_us.html",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us! The currently open positions are listed below. It might also be possible for us to offer opportunities for internships in data science (analysis of large scale behavioral data, computational modeling), game design and development, computer engineering (web apps), and digital arts for video games (graphics, animations, sound).\nFeel free to get in touch with us at contact@xcit.org to find out more.\n\n\n\nThere are currently 2 fully-funded PhD candidate positions open in the xCIT lab.\n\n  Learn more about the PhD journey at the University of Luxembourg\n\n\n\n\n  Learn more about this position\n\n\n\n\nThis is a position funded by the SnT with whom we are collaborating with on a project that aims to assess teams’s readiness level to handle various forms of cyber attacks.\n\n  Learn more about this position",
    "crumbs": [
      "About",
      "Join Us"
    ]
  },
  {
    "objectID": "join_us.html#join-us",
    "href": "join_us.html#join-us",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us! The currently open positions are listed below. It might also be possible for us to offer opportunities for internships in data science (analysis of large scale behavioral data, computational modeling), game design and development, computer engineering (web apps), and digital arts for video games (graphics, animations, sound).\nFeel free to get in touch with us at contact@xcit.org to find out more.\n\n\n\nThere are currently 2 fully-funded PhD candidate positions open in the xCIT lab.\n\n  Learn more about the PhD journey at the University of Luxembourg\n\n\n\n\n  Learn more about this position\n\n\n\n\nThis is a position funded by the SnT with whom we are collaborating with on a project that aims to assess teams’s readiness level to handle various forms of cyber attacks.\n\n  Learn more about this position",
    "crumbs": [
      "About",
      "Join Us"
    ]
  },
  {
    "objectID": "join_us.html#master-and-bachelor-students-projects",
    "href": "join_us.html#master-and-bachelor-students-projects",
    "title": "xCIT Lab",
    "section": "Master and Bachelor students projects",
    "text": "Master and Bachelor students projects\nThere are several ways to join the xCIT research group as a Master or Bachelor student. In each case you should contact us and send us your CV as well as samples of your past work (e.g., code on GitHub, Bachelor thesis).\n\nStudent assistant job\nThis is a paid position, applicable only for students at the Univeristy of Luxembourg, and corresponding to a maximum workload of 40H/week.\nWe typically only accept one to two student assistants per year. If you have good programming skills (e.g., in Python) and are autonomous and well-organized, your chances are better.\n\n\nInternships\nWe can support students who want to complete an internship in our lab. Internships can last somewhere between 2 and 6 months.\n\n\nBachelor or Master thesis\nIf you are enrolled at the University of Luxembourg and would like to do your Bachelor or Master thesis research work with us, we might be able to suggest research topics and support you in completing that research work.",
    "crumbs": [
      "About",
      "Join Us"
    ]
  },
  {
    "objectID": "join_us.html#citizen-science",
    "href": "join_us.html#citizen-science",
    "title": "xCIT Lab",
    "section": "Citizen Science",
    "text": "Citizen Science\nIf you have technical, artistic or other skills that you would like to put to use for science, feel free to reach out; there is always more work and ideas than hands to build stuff;-).",
    "crumbs": [
      "About",
      "Join Us"
    ]
  },
  {
    "objectID": "join_us.html#other-jobs",
    "href": "join_us.html#other-jobs",
    "title": "xCIT Lab",
    "section": "Other jobs",
    "text": "Other jobs\nThe University of Luxembourg is a great place for researchers, and there are often great job opportunities. You can learn about open positions at the University of Luxembourg.\nLearn more about Jobs available at UniLu",
    "crumbs": [
      "About",
      "Join Us"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "xCIT Lab",
    "section": "",
    "text": "Home\n    Research",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research.html#research-topics",
    "href": "research.html#research-topics",
    "title": "xCIT Lab",
    "section": "Research Topics",
    "text": "Research Topics\n\nCognitive sciences\nThe main research focus of xCIT is cognitive science: which includes psychology, cognitive neurosciences and computer sciences (AI).\nWe use the methods, theories and tools from these fields to gain insights about the human mind and brain, and then to use those insights to possibly create new tools and theories.\nWe are currently actively working on cognitve assessment (REF) and cognitive training (REF). We are also working on\nusing insights and methods across fields like experimental psychology, neuroscience and AI to study the human mind.\nWe are working on cognitive training with video games Studying collaboration skills\n\n\nScientific methods, tools and practices\nWe invest a large chunk of our time in developing methods, tools and practices to improve how we do cognitive science research. standards, api, etc\nYou can learn more about this aspect of our work on behaverse.org\n\n\nDesign and Development\nUX software dev prototyping game design and dev (in collaboration with external partners)\n\n\nApplication domains for societal impact\nCognitive training, Human learning, Mental health, Sustainability Cybersecurity",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research.html#research-methods",
    "href": "research.html#research-methods",
    "title": "xCIT Lab",
    "section": "Research Methods",
    "text": "Research Methods\nWe use a variety of methods; in general, we aim to use the best tools for the situation at hand rather than to find problems that can be addressed with the tools we already have.\n\nExperimental psychology\nThe main research approach involves conducting behavioral experiments where we collect data from participants engaged in various activities.\n\n\nVideo games design and development\n\n\nComputational methods and Applied Machine Learning\n\n\nUX design",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "collaborations.html",
    "href": "collaborations.html",
    "title": "Collaborations",
    "section": "",
    "text": "We enjoy collaborating with experts from a diversity of backgrounds and institutions. Our collaborative approach enriches our research and extends our impact."
  },
  {
    "objectID": "collaborations.html#long-standing-collaborators",
    "href": "collaborations.html#long-standing-collaborators",
    "title": "Collaborations",
    "section": "Long-Standing Collaborators",
    "text": "Long-Standing Collaborators\n\nAcademic Partners\n\nDaphne Bavelier - [Add affiliation and collaboration focus]\nPaul Schrater - [Add affiliation and collaboration focus]\nFlorian Waszak - [Add affiliation and collaboration focus]\nEmmanuel Schmuck - [Add affiliation and collaboration focus]"
  },
  {
    "objectID": "collaborations.html#institutional-partnerships",
    "href": "collaborations.html#institutional-partnerships",
    "title": "Collaborations",
    "section": "Institutional Partnerships",
    "text": "Institutional Partnerships\n\nUniversity of Luxembourg\n[Add information about internal collaborations]\n\n\nInternational Collaborations\n[Add information about international research partnerships]"
  },
  {
    "objectID": "collaborations.html#industry-partnerships",
    "href": "collaborations.html#industry-partnerships",
    "title": "Collaborations",
    "section": "Industry Partnerships",
    "text": "Industry Partnerships\n[Add information about industry collaborations and technology transfer]"
  },
  {
    "objectID": "collaborations.html#collaborative-projects",
    "href": "collaborations.html#collaborative-projects",
    "title": "Collaborations",
    "section": "Collaborative Projects",
    "text": "Collaborative Projects\nFor specific research projects involving collaborations, see our Projects page."
  },
  {
    "objectID": "collaborations.html#interested-in-collaborating",
    "href": "collaborations.html#interested-in-collaborating",
    "title": "Collaborations",
    "section": "Interested in Collaborating?",
    "text": "Interested in Collaborating?\nWe’re always open to new collaborative opportunities. If you’re interested in working with us, please get in touch with a brief description of your research interests and potential collaboration ideas."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "This page was auto-generated using data from Google Scholar.\n\n2025\nSonnleitner, P., Bernard, S., Michels, M.A. et al. (2025). Establishing Cognitive Item Models for Fair and Theory-Grounded Automatic Item Generation: A Large-Scale Assessment Study with Image-Based Math Items. Applied Measurement in Education, 1-23.\nYu, H. & Cardoso-Leite, P. (2025). Prototyping Video Games for Understanding Abstract Concepts. 2025 IEEE Conference on Games (CoG), 1-4. OA\nDarriba, Á., Munnich, A., Cardoso-Leite, P. et al. (2025). Clinical and cognitive assessment in Friedreich ataxia clinical trials: a review. Frontiers Media SA, 16, 1558493. OA\n\n\n2024\nMukherjee, S., Distler, V., Lenzini, G. et al. (2024). Balancing the perception of cheating detection, privacy and fairness: A mixed-methods study of visual data obfuscation in remote proctoring.. OA\nMaquil, V., Afkari, H., Arend, B. et al. (2024). Analysis of coordination mechanisms during collaborative problem-solving on an interactive tabletop display. Computer Supported Cooperative Work (CSCW), 33(4), 1071-1113. OA\nParis, A., Labetoulle, A., Bezjak, S. et al. (2024). AI4T-Comparative European Evaluation Report.. OA\n\n\n2023\nSunnen, P., Arend, B., Heuser, S. et al. (2023). ‘How Do We Move Back?’–A Case Study of Joint Problem-Solving at an Interactive Tabletop Mediated Activity. Springer Nature Switzerland, 149-162. OA\nYu, H. & Cardoso-Leite, P. (2023). Video games to study and improve collaboration skills.. OA\nAnsarinia, M. (2023). Towards a Computational Model of General Cognitive Control Using Artificial Intelligence, Experimental Psychology and Cognitive Neuroscience.. OA\nFERNANDEZ, P.I.I., MICHELS, M.A., HORNUNG, C. et al. (2023). The impact of cognitive characteristics and image-based semantic embeddings on item difficulty. Annual Meeting of the National Council on Measurement in Education. OA\nRad, M.S., Ansarinia, M. & Shafir, E. (2023). Temporary self-deprivation can impair cognitive control: Evidence from the Ramadan fast. Personality and social psychology bulletin, 49(3), 415-428. OA\nAfkari, H. & Bednarik, R. (2023). Seize the moment: the role of scrub nurses’ proactivity in microsurgical operating-room collaborations. Behaviour & Information Technology, 42(10), 1640-1657. OA\nMaquil, V., Anastasiou, D., Afkari, H. et al. (2023). Establishing awareness through pointing gestures during collaborative decision-making in a wall-display environment.. OA\nFarahzadi, Y., Ansarinia, M. & Kekecs, Z. (2023). Decoding Hypnotic Experience from Raw EEG using a Multi-Output Auto-Encoder.. OA\nAnsarinia, M. & Cardoso-Leite, P. (2023). CogPonder: Towards a Computational Framework of General Cognitive Control. 2023 Conference on Cognitive Computational Neuroscience. OA\nParis, A., Labetoulle, A., Chesné, J. et al. (2023). AI4T National Evaluation Report-France.. OA\n\n\n2022\nMICHELS, M.A., HORNUNG, C., GAMO, S. et al. (2022). Validierung und psychometrische Analyse automatisch generierter Mathematikaufgaben im Bereich Zahlen und Operationen. 9. GEBF-Tagung. OA\nMICHELS, M.A., HORNUNG, C., GAMO, S. et al. (2022). Validation and Psychometric Analysis of 32 cognitive item models spanning Grades 1 to 7 in the mathematical domain of numbers & operations. Luxembourg Educational Research Association Conference 2022. OA\nCastañón, S.H., Cardoso-Leite, P., Green, C.S. et al. (2022). Optimal learning under structural environmental uncertainty reveals inherent learning trade-offs. 44th Annual Meeting of the Cognitive Science Society: Cognitive Diversity, CogSci 2022. OA\nAnsarinia, M., Schrater, P. & Cardoso-Leite, P. (2022). Linking theories and methods in cognitive sciences via joint embedding of the scientific literature: The example of cognitive control. arXiv preprint arXiv:2203.11016. OA\nAnastasiou, D., Ion, R., Badea, V. et al. (2022). ENRICH4ALL: A first Luxembourgish BERT Model for a Multilingual Chatbot.. OA\nVandenabeele, L., Afkari, H., Hermen, J. et al. (2022). Deborah: a web-based cross-device orchestration layer.. OA\nAnsarinia, M., Clocher, B., Defossez, A. et al. (2022). Cogenv: A reinforcement learning environment for cognitive tests. 2022 Conference on Cognitive Computational Neuroscience. OA\nAnastasiou, D., Ruge, A., Ion, R. et al. (2022). A machine translation-powered chatbot for public administration. Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, 329-330. OA\n\n\n2021\nMaquil, V., Afkari, H., Arend, B. et al. (2021). Research Article Balancing Shareability and Positive Interdependence to Support Collaborative Problem-Solving on Interactive Tabletops.. OA\nRad, M.S., Ansarinia, M. & Ginges, J. (2021). Ramadan, Risk-taking, and Cooperation in Resource Dilemmas: Communal Deprivation as a Cultural Response to Scarcity?.. OA\nSunnen, P., Arend, B., Heuser, S. et al. (2021). Orbitia–Gemeinsam auf Mission.. University of Luxembourg, Esch-sur-Alzette, Unknown/unspecified. OA\nCardoso-Leite, P., Buchard, A., Tissieres, I. et al. (2021). Media use, attention, mental health and academic performance among 8 to 12 year old children. PloS one, 16(11), e0259163. OA\nFischbach, A., Greiff, S., CARDOSO-LEITE, P. et al. (2021). Digitalisierung der pädagogischen Diagnostik: Von Evolution zu Revolution. University of Luxembourg, Esch-sur-Alzette, Luxembourg. OA\nFischbach, A., Greiff, S., CARDOSO-LEITE, P. et al. (2021). Digitalisation du diagnostic pédagogique: De l’évolution à la révolution. University of Luxembourg, Esch-sur-Alzette, Luxembourg. OA\nMaquil, V., Afkari, H., Arend, B. et al. (2021). Balancing Shareability and Positive Interdependence to Support Collaborative Problem‐Solving on Interactive Tabletops. Advances in Human‐Computer Interaction, 2021(1), 6632420. OA\nCastañón, S.H., Cardoso-Leite, P., Altarelli, I. et al. (2021). A mixture of generative models strategy helps humans generalize across tasks. BioRxiv, 2021.02. 16.431506. OA\n\n\n2020\nAnastasiou, D., Afkari, H. & Maquil, V. (2020). “You move THIS!”: Annotation of Pointing Gestures on Tabletop Interfaces in Low Awareness Situations. Proceedings of LREC2020 Workshop” People in language, vision and the mind”(ONION2020), 22-27. OA\nCardoso-Leite, P., Ansarinia, M., Schmück, E. et al. (2020). Training cognition with video games.. OA\nDefossez, A., Ansarinia, M., Clocher, B. et al. (2020). The structure of behavioral data. arXiv preprint arXiv:2012.12583. OA\nLEVY, J., MUSSACK, D., Brunner, M. et al. (2020). Tackling educational inequalities using school effectiveness measures. (Semi) virtual LuxERA Conference 2020. OA\nAfkari, H., Maquil, V. & Anastasiou, D. (2020). Exploring Opportunities of Tabletop Interfaces for Promoting and Analysing Collaboration.. ETIS. OA\nSunnen, P., Arend, B., Heuser, S. et al. (2020). Developing an interactive tabletop mediated activity to induce collaboration by implementing design considerations based on cooperative learning principles. Springer International Publishing, 316-324. OA\nAfkari, H., Maqui, V., Arend, B. et al. (2020). Designing different features of an interactive tabletop application to support collaborative problem-solving.. OA\nLevy, J., Mussack, D., Brunner, M. et al. (2020). Contrasting classical and machine learning approaches in the estimation of value-added scores in large-scale educational data. Frontiers in psychology, 11, 2190. OA\nLEVY, J., MUSSACK, D., Brunner, M. et al. (2020). Can machine learning methods lead to more precise measures of school effectiveness? An application of various machine learning approaches in the estimation of school value-added scores. 12th Conference of the International Test Commission. OA\nCardoso-Leite, P., Joessel, A. & Bavelier, D. (2020). 18 Games for Enhancing Cognitive Abilities. Handbook of game-based learning, 437. OA\nAREND, B., SUNNEN, P., HEUSER, S. et al. (2020). ‘Being a space mining crew’: How participants jointly discover their complementary resources while engaging into a serious game at an interactive tabletop. L. Gomez Chova, A. Lopez Martinez, I. Candel Torres: Proceedings, International Conference on Education and New Learning Technologies 2020. OA\n\n\n2019\nMussack, D., Flemming, R., Schrater, P. et al. (2019). Towards discovering problem similarity through deep learning: combining problem features and user behavior.. Proceedings of the 12th international conference on educational data mining (EDM 2019). OA\nSchmück, E., Flemming, R., Schrater, P. et al. (2019). Principles underlying the design of a cognitive training game as a research framework. 2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games), 1-2. OA\nSunnen, P., Arend, B., Heuser, S. et al. (2019). Designing collaborative scenarios on tangible tabletop interfaces-insights from the implementation of paper prototypes in the context of a multidisciplinary design workshop. Proceedings of 17th European Conference on Computer-Supported Cooperative Work. OA\nMaquil, V., Afkari, H., Moll, C. et al. (2019). Active tangibles for tabletop interaction based on the Kniwwelino prototyping platform.. OA\nFlemming, R., Schmück, E., Mussack, D. et al. (2019). A generalizable performance evaluation model of driving games via risk-weighted trajectories. Proceedings of The 12th International Conference on Educational Data Mining (EDM 2019), 548, 551. OA\nANSARINIA, M., Mussack, D., Schrater, P. et al. (2019). A Multi-Objective Optimization Algorithm to Generate Unbiased Stimuli Sequences for Cognitive Tasks. Bernstein Conference 2019. OA\nAnsarinia, M., Mussack, D., Schrater, P. et al. (2019). A Formal Framework for Structured N-Back Stimuli Sequences. Conference on Cognitive Computational Neuroscience. OA"
  },
  {
    "objectID": "people/past_members/mussac_dominic.html",
    "href": "people/past_members/mussac_dominic.html",
    "title": "Dominic Mussac",
    "section": "",
    "text": "Home\n    People\n    PhD Candidates\n    Dominic Mussac"
  },
  {
    "objectID": "people/past_members/mussac_dominic.html#hi",
    "href": "people/past_members/mussac_dominic.html#hi",
    "title": "Dominic Mussac",
    "section": "Hi!",
    "text": "Hi!\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "people/past_members/clocher_brice.html",
    "href": "people/past_members/clocher_brice.html",
    "title": "Brice Clocher",
    "section": "",
    "text": "Home\n    People\n    Research Staff\n    Brice Clocher"
  },
  {
    "objectID": "people/past_members/clocher_brice.html#bio",
    "href": "people/past_members/clocher_brice.html#bio",
    "title": "Brice Clocher",
    "section": "Bio",
    "text": "Bio\nFascinated by digital media at a young age, Brice completed a M.Sc. in Computer Science and Engineering (INSA Lyon) with the firm intention to create experiences that are meaningful and benefit society. His belief in a holistic and multidisciplinary approach to creativity led him to hone his crafts in various European start-ups and universities, producing a wide range of digital systems from tangible crisis management tools to alternate reality games. After teaching game development as an artistic practice to design students, he co-funded a game studio (Bunny & Gnome) and released an educational game about trash sorting which won four awards and was exhibited in a digital art museum (Trash Monsters)."
  },
  {
    "objectID": "people/pi/cardosoleite_pedro.html",
    "href": "people/pi/cardosoleite_pedro.html",
    "title": "Pedro Cardoso-Leite",
    "section": "",
    "text": "Home\n    People\n    Pedro Cardoso-Leite"
  },
  {
    "objectID": "people/pi/cardosoleite_pedro.html#hi",
    "href": "people/pi/cardosoleite_pedro.html#hi",
    "title": "Pedro Cardoso-Leite",
    "section": "Hi!",
    "text": "Hi!\nI’m a cognitive scientist, experimental psychologist, data/tech/games enthusiast and tinkerer. I use an interdisciplinary, multi/mixed-method approach to design and leverage digital technologies–in particular, video games–to study the human mind and aim to improve people’s lives.\nI believe that as (cognitive) scientists we should contribute to solving major societal challenges, including education, cognitive and mental health, climate change and cybersecurity. And to have such an impact we need to improve how cognitive science is done, by supporting for instance the development, deployment and testing of behavioral interventions at scale.\nI studied psychology and cognitive sciences and received my PhD in experimental psychology from Paris Descartes University (now named Université Paris Cité). As a postdoc, I was trained in brain imaging at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig. After that, I joined the University of Geneva to work with Daphne Bavelier on the impact of digital technologies, in particular video games, on cognitive abilities, learning and transfer. I’m now an associate Professor in Cognitive Sciences and Assessment in the Department of Behavioral and Cognitive Sciences at the University of Luxembourg. I’m also the head of the xCIT research group and of the Userlab."
  },
  {
    "objectID": "people/members/ansarinia_morteza.html",
    "href": "people/members/ansarinia_morteza.html",
    "title": "Morteza Ansarinia",
    "section": "",
    "text": "Home\n    People\n    Morteza Ansarinia"
  },
  {
    "objectID": "people/members/ansarinia_morteza.html#hello",
    "href": "people/members/ansarinia_morteza.html#hello",
    "title": "Morteza Ansarinia",
    "section": "Hello!",
    "text": "Hello!\nI’m Morteza, a researcher at the University of Luxembourg. I specialize in computational cognitive neuroscience and computer engineering, and enjoy working on projects that have an impact on aligning machines to human cognition (or the other way around).\nI conducted my PhD in Behavioral and Cognitive Sciences at the University of Luxembourg (Belval, Luxembourg) in partnership with Max Planck Institute for Human Cognitive and Brain Sciences (Leipzig, Germany); my thesis emphasizes a computationally grounded perspective on cognitive control in the human brain.\nI’m currently developing tools for automated scientific workflows and large-scale data lakehouses, specifically designed for multimodal cognitive data. My primary interest is resting-state brain–when the brain is not engaged in any specific task."
  },
  {
    "objectID": "people/members/afkari_hoorieh.html",
    "href": "people/members/afkari_hoorieh.html",
    "title": "Hoorieh Afkari",
    "section": "",
    "text": "Home\n    People\n    Hoorieh Afkari"
  },
  {
    "objectID": "people/members/afkari_hoorieh.html#hello-im-hoorieh",
    "href": "people/members/afkari_hoorieh.html#hello-im-hoorieh",
    "title": "Hoorieh Afkari",
    "section": "1. Hello, I’m Hoorieh",
    "text": "1. Hello, I’m Hoorieh\nI explore the space where people meet technology. As an HCI researcher and R&D professional, I’m driven by a curiosity about human behavior and a passion for designing digital systems that respond to real needs, not just technical specs."
  },
  {
    "objectID": "people/members/afkari_hoorieh.html#what-im-working-on-and-why",
    "href": "people/members/afkari_hoorieh.html#what-im-working-on-and-why",
    "title": "Hoorieh Afkari",
    "section": "2. What I’m Working On (and Why)",
    "text": "2. What I’m Working On (and Why)\nRight now, at the University of Luxembourg, I explore how digital technologies can support and enhance human learning and behavior. At the heart of it, I care about impact: creating technology that makes a real difference in people’s lives."
  },
  {
    "objectID": "people/members/afkari_hoorieh.html#the-journey",
    "href": "people/members/afkari_hoorieh.html#the-journey",
    "title": "Hoorieh Afkari",
    "section": "3. The Journey",
    "text": "3. The Journey\nI completed my PhD in Human-Computer Interaction in Finland, focusing on medical technologies. My PhD thesis, The Potentials for Gaze-Based Interaction with the Surgical Microscope, is available here.\nSince then, I’ve worked in applied research and development across UX and digital tools; work that led me to Luxembourg, where I continue exploring how people interact with technology."
  },
  {
    "objectID": "people/members/afkari_hoorieh.html#beyond-work",
    "href": "people/members/afkari_hoorieh.html#beyond-work",
    "title": "Hoorieh Afkari",
    "section": "4. Beyond Work",
    "text": "4. Beyond Work\nUnderstanding people has always been more than just part of my work; it’s something I genuinely care about. That’s why I pursued a coaching certification at the University of Cambridge, where I’ve built on my skills in listening, empathy, and communication. Outside of work, I enjoy learning new languages, reading philosophy, and going for long walks. Traveling helps me see the world from different perspectives and reminds me how differently people live, think, and connect."
  },
  {
    "objectID": "people/members/yu_hainan.html",
    "href": "people/members/yu_hainan.html",
    "title": "Hainan Yu",
    "section": "",
    "text": "Home\n    People\n    Hainan Yu"
  },
  {
    "objectID": "people/members/yu_hainan.html#hi",
    "href": "people/members/yu_hainan.html#hi",
    "title": "Hainan Yu",
    "section": "Hi!",
    "text": "Hi!\nI’m Hainan, a PhD student in Psychology under the supervision of Prof. Dr. Pedro Cardoso-Leite. My research focuses on human collaboration—its components, dynamics, and skills, and how we can observe and study it in practice. To support this, we are developing a framework designed to systematically analyze collaboration and building a proto-theory to better understand collaboration and its skills.\nA core part of our work involves using video games as research environments and tools. Drawing on our framework and applying game design methods, we develop games to test and refine our proto-theory. This approach allows us to dive deeply into collaboration in a controlled yet engaging setting. Our research brings together psychology, computer science, and game research, and I value its interdisciplinary nature.\nI have a background in data science, reinforcement learning, and game AI, and I’m particularly interested in how video games and technology can support learning and human interaction. In my spare time, I’m a game designer, developer, and music producer. I also enjoy exploring how video games convey emotions and create impact through storytelling and game mechanisms. I believe that through thoughtful design, games and technology can help us better understand ourselves, each other, and the world around us."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html",
    "href": "publications/proceedings/girard2015d.html",
    "title": "How much training data for facial action unit detection?",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html#citation-apa-7",
    "href": "publications/proceedings/girard2015d.html#citation-apa-7",
    "title": "How much training data for facial action unit detection?",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html#abstract",
    "href": "publications/proceedings/girard2015d.html#abstract",
    "title": "How much training data for facial action unit detection?",
    "section": "Abstract",
    "text": "Abstract\nBy systematically varying the number of subjects and the number of frames per subject, we explored the influence of training set size on appearance and shape-based approaches to facial action unit (AU) detection. Digital video and expert coding of spontaneous facial activity from 80 subjects (over 350,000 frames) were used to train and test support vector machine classifiers. Appearance features were shape-normalized SIFT descriptors and shape features were 66 facial landmarks. Ten-fold cross-validation was used in all evaluations. Number of subjects and number of frames per subject differentially affected appearance and shape-based classifiers. For appearance features, which are high-dimensional, increasing the number of training subjects from 8 to 64 incrementally improved performance, regardless of the number of frames taken from each subject (ranging from 450 through 3600). In contrast, for shape features, increases in the number of training subjects and frames were associated with mixed results. In summary, maximal performance was attained using appearance features from large numbers of subjects with as few as 450 frames per subject. These findings suggest that variation in the number of subjects rather than number of frames per subject yields most efficient performance."
  },
  {
    "objectID": "publications/articles/baber2024.html",
    "href": "publications/articles/baber2024.html",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "",
    "text": "Baber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (in press). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep."
  },
  {
    "objectID": "publications/articles/baber2024.html#citation-apa-7",
    "href": "publications/articles/baber2024.html#citation-apa-7",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "",
    "text": "Baber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (in press). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep."
  },
  {
    "objectID": "publications/articles/baber2024.html#abstract",
    "href": "publications/articles/baber2024.html#abstract",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "Abstract",
    "text": "Abstract\nTBA"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html",
    "href": "publications/articles/adaryukov2024.html",
    "title": "Worth the weight: An examination of unstructured and structured data in graduate admissions",
    "section": "",
    "text": "Adaryukov, J., Biernat, M., Girard, J. M., Villicana, A. J., & Pleskac, T. J. (in press). Worth the weight: An examination of unstructured and structured data in graduate admissions. Decision."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#citation-apa-7",
    "href": "publications/articles/adaryukov2024.html#citation-apa-7",
    "title": "Worth the weight: An examination of unstructured and structured data in graduate admissions",
    "section": "",
    "text": "Adaryukov, J., Biernat, M., Girard, J. M., Villicana, A. J., & Pleskac, T. J. (in press). Worth the weight: An examination of unstructured and structured data in graduate admissions. Decision."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#abstract",
    "href": "publications/articles/adaryukov2024.html#abstract",
    "title": "Worth the weight: An examination of unstructured and structured data in graduate admissions",
    "section": "Abstract",
    "text": "Abstract\nIn graduate admissions, as in many multiattribute decisions, evaluators must judge candidates from a flood of information, including recommendation letters, personal statements, grades, and standardized test scores. Some of this information is structured, while some is unstructured. Yet most studies of multiattribute decisions focus on decisions made from structured information. This study evaluated how structured and unstructured information is used within graduate admissions decisions. We examined a uniquely comprehensive dataset of N = 2, 231 graduate applications to the University of Kansas, containing full application packages, demographics, and final admissions decisions for each applicant. To make sense of our documents, we applied structural topic modeling (STM), a topic model that allows topic content and prevalence to covary based on other metadata (e.g., department of study). STM allowed us to examine what information the letters and statements contain, and the relationships between variables like gender and race and the textual information. We found that most topics in the unstructured data related to specific fields of study. The STMs did not uncover strong differences among applicants regarding race and gender though the recommendation letters and personal statements for international applicants did show some different topic profiles than domestic applicants. We also found that admissions decision-makers behaved as if they prioritized structured numeric metrics, using unstructured information to check for disqualifications, if at all. However, we found that topics were less reliable than admissions documents, meaning that additional ways of using them cannot be completely ruled out. The implications of our findings on graduate admissions decisions are discussed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to xCIT!",
    "section": "",
    "text": "We are xCIT—a small, interdisciplinary research group at the University of Luxembourg that aims to advance cognitive sciences, using and developing innovative technologies for positive societal impact.\nTogether with our collaborators, we combine a broad range of skills and expertise, covering in particular, cognitive science, artificial intelligence, UX, software engineering, cybersecurity, game design and development.\nWe conduct experiments with human participants, using a wide range of instruments (e.g., cognitive tests, video games) and approaches (e.g., online computerized tests, brain imaging, interviews) to collect large and/or rich datasets. We then use a variety of data analytic methods to gain insights about various facets of the human mind.\nWe also design and develop new technologies. This includes various interactive digital artefacts designed to collect data (e.g., questionnaires, cognitive assessments, video games), to affect people (e.g., cognitive training with video games) or to improve cognitive science research (e.g., defining behavioral data standards, studyflow modeler). To design, develop and validate those technologies we use again a large range of methods, tools and approaches, including for instance iterative design and user testing.\nWe tackle several ambitious long term projects, because we believe they are important and can have an impact. In particular we aim to build up critical capacities to scale up cognitive science research and take full advantage of modern AI tools.\nWe value collaboration, open science, and human-centered design. We believe that this unique mixed-method, interdisciplinary, open-science approach is critical if we are to make sense of our complex world and design interventions and tools that benefit humans."
  },
  {
    "objectID": "index.html#learn-more-about-our-work",
    "href": "index.html#learn-more-about-our-work",
    "title": "Welcome to xCIT!",
    "section": "Learn More About Our Work",
    "text": "Learn More About Our Work\n\n\nResearch Areas\nExplore our cognitive science research, including cognitive assessment, training with video games, and collaborative skills.\n\n\nCurrent Projects\nDiscover our active research projects including Behaverse and CERISE.\n\n\nPublications\nBrowse our recent research publications and scientific contributions.\n\n\nTeaching\nLearn about our courses in computational cognitive sciences and data science.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Welcome to xCIT!",
    "section": "Get Involved",
    "text": "Get Involved\nInterested in joining the lab? Check out our open positions for PhD candidates, postdocs, and student opportunities.\nWant to collaborate? Learn about our research partnerships and get in touch.\nStudents looking for projects? Explore potential thesis topics and internships.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us!\nFeel free to get in touch with us at contact@xcit.org\n\nThe xCIT Lab is located in the Belval Campus of the University of Luxembourg: you can find us in the “Maison des Sciences Humaines”.",
    "crumbs": [
      "About",
      "Contact"
    ]
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us!\nFeel free to get in touch with us at contact@xcit.org\n\nThe xCIT Lab is located in the Belval Campus of the University of Luxembourg: you can find us in the “Maison des Sciences Humaines”.",
    "crumbs": [
      "About",
      "Contact"
    ]
  },
  {
    "objectID": "publications/articles/bowdring2021.html",
    "href": "publications/articles/bowdring2021.html",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/bowdring2021.html#citation-apa-7",
    "href": "publications/articles/bowdring2021.html#citation-apa-7",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/bowdring2021.html#abstract",
    "href": "publications/articles/bowdring2021.html#abstract",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "Abstract",
    "text": "Abstract\nPhysical attractiveness plays a central role in psychosocial experiences. One of the top research priorities has been to identify factors affecting perceptions of physical attractiveness (PPA). Recent work suggests PPA derives from different sources (e.g., target, perceiver, stimulus type). Although smiles in particular are believed to enhance PPA, support has been surprisingly limited. This study comprehensively examines the effect of smiles on PPA and, more broadly, evaluates the roles of target, perceiver, and stimulus type in PPA variation. Perceivers (n=181) rated both static images and 5-s videos of targets displaying smiling and neutral-expressions. Smiling images were rated as more attractive than neutral-expression images (regardless of stimulus motion format). Interestingly, perceptions of physical attractiveness were based more on the perceiver than on either the target or format in which the target was presented. Results clarify the effect of smiles, and highlight the significant role of the perceiver, in PPA."
  },
  {
    "objectID": "publications/articles/butler2024.html",
    "href": "publications/articles/butler2024.html",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "publications/articles/butler2024.html#citation-apa-7",
    "href": "publications/articles/butler2024.html#citation-apa-7",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "publications/articles/butler2024.html#abstract",
    "href": "publications/articles/butler2024.html#abstract",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "Abstract",
    "text": "Abstract\nObjective: Imaginal exposure is a novel intervention for eating disorders (EDs) that has been investigated as a method for targeting ED symptoms and fears. Research is needed to understand mechanisms of change during imaginal exposure for EDs, including whether within- and between-session distress reduction is related to treatment outcomes.\nMethod: Study 1 tested four sessions of online imaginal exposure (N = 143). Study 2 examined combined imaginal and in vivo exposure, comprising six imaginal exposure sessions (N = 26). ED symptoms and fears were assessed pre- and posttreatment, and subjective distress and state anxiety were collected during sessions.\nResults: Subjective distress tended to increase within-session in both studies, and within-session reduction was not associated with change in ED symptoms or fears. In Study 1, between-session reduction of distress and state anxiety was associated with greater decreases in ED symptoms and fears pre-to posttreatment. In Study 2, between-session distress reduction occurred but was not related to outcomes.\nConclusions: Within-session distress reduction may not promote change during exposure for EDs, whereas between-session distress reduction may be associated with better treatment outcomes. These findings corroborate research on distress reduction during exposure for anxiety disorders. Clinicians might consider approaches to exposure-based treatment that focus on distress tolerance and promote between-session distress reduction."
  },
  {
    "objectID": "publications/proceedings/girard2011.html",
    "href": "publications/proceedings/girard2011.html",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#citation-apa-7",
    "href": "publications/proceedings/girard2011.html#citation-apa-7",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#abstract",
    "href": "publications/proceedings/girard2011.html#abstract",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "Abstract",
    "text": "Abstract\nImplementing a computerized facial expression analysis system for automatic coding requires that a threshold for the system’s classifier outputs be selected. However, there are many potential ways to select a threshold. How do different criteria and metrics compare? Manually FACS coded video of 45 clinical interviews (Spectrum dataset) were processed using person-specific active appearance models (AAM). Support vector machine (SVM) classifiers were trained using an independent dataset (RU-FACS). Spectrum sessions were randomly assigned to training (n=32) and testing sets (n=13). Six different threshold selection criteria were compared for automatic AU coding. Three major findings emerged: 1) Thresholds that attempt to balance the confusion matrix (using kappa, \\(F_1\\), or MCC) performed significantly better on all metrics than thresholds that select arbitrary error or accuracy rates (such as TPR, FPR, or EER). 2) AU detection scores for kappa, \\(F_1\\), and MCC were highly intercorrelated; accuracy was uncorrelated with the others. And 3) Kappa, MCC, and \\(F_1\\) were all positively correlated with base rate. They increased with increases in AU base rates. Accuracy, by contrast, showed the opposite pattern. It was strongly negatively correlated with base rate. These findings suggest that better automatic coding can be obtained by using threshold-selection criteria that balance the confusion matrix and benefit from increased AU base rates in the training data."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#author-note",
    "href": "publications/proceedings/girard2011.html#author-note",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "Author Note",
    "text": "Author Note\n\nWhen I wrote this paper back in 2011, I was just learning about performance evaluation. This was a first, and rather naive attempt at understanding the connection between agreement, prevalence, and threshold selection. Readers interested in more sophisticated approaches to these issues are encouraged to look up Guangchao Charles Feng, who has done nice work in this area.\nr tufte::quote_footer('--- Jeffrey Girard, 2018-06-14')"
  },
  {
    "objectID": "publications/proceedings/girard2013.html",
    "href": "publications/proceedings/girard2013.html",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2013.html#citation-apa-7",
    "href": "publications/proceedings/girard2013.html#citation-apa-7",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2013.html#abstract",
    "href": "publications/proceedings/girard2013.html#abstract",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "Abstract",
    "text": "Abstract\nInvestigated the relationship between change over time in severity of depression symptoms and facial expression. Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews. Facial expressions were analyzed from the video using both manual and automatic systems. Automatic and manual coding were highly consistent for FACS action units, and showed similar effects for change over time in depression severity. For both systems, when symptom severity was high, participants made more facial expressions associated with contempt, smiled less, and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt. These results are consistent with the “social risk hypothesis” of depression. According to this hypothesis, when symptoms are severe, depressed participants withdraw from other people in order to protect themselves from anticipated rejection, scorn, and social exclusion. As their symptoms fade, participants send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science."
  },
  {
    "objectID": "people/members/mukherjee_suvadeep.html",
    "href": "people/members/mukherjee_suvadeep.html",
    "title": "Suvadeep Mukherjee",
    "section": "",
    "text": "Home\n    People\n    Suvadeep Mukherjee"
  },
  {
    "objectID": "people/members/mukherjee_suvadeep.html#hi",
    "href": "people/members/mukherjee_suvadeep.html#hi",
    "title": "Suvadeep Mukherjee",
    "section": "Hi!",
    "text": "Hi!\nI’m a doctoral researcher in psychology at the University of Luxembourg supervised by Prof. Cardoso-Leite. My work focuses on designing online examination systems that are both secure and user-friendly. We are designing digital strategies that effectively prevent student cheating during exams while upholding strong privacy protections and ensuring a smooth, positive test-taking experience - addressing key components of a fair and valid exam.\nI take an interdisciplinary, mixed-method approach to designing and testing behavioral interventions. Using realistic and interactive interfaces, I explore how these interventions influence students’ behavior, ultimately aiming to develop digital exam systems that are fair, effective and privacy-conscious.\nWith my background in Computer Science and Human-Computer Interaction, designing digital solutions that meet real user needs is my sweet spot. I’m also an AI and data science enthusiast, passionate about integrating cutting-edge tech into systems that positively impact people’s lives."
  },
  {
    "objectID": "people/members/doublet_sophie.html",
    "href": "people/members/doublet_sophie.html",
    "title": "Sophie Doublet",
    "section": "",
    "text": "Home\n    People\n    Sophie Doublet"
  },
  {
    "objectID": "people/members/doublet_sophie.html#hi",
    "href": "people/members/doublet_sophie.html#hi",
    "title": "Sophie Doublet",
    "section": "Hi!",
    "text": "Hi!\nI’m Sophie, a UX researcher at the University of Luxembourg. I’m passionate about understanding how people think, feel, and interact with the world—particularly when it involves technology. My primary focus is to understand people’s need to design meaningful and ethical interactions.\nI use a variety of research methods—such as user testing, surveys, and interviews for the classical ones—to ensure the user’s voice is heard throughout the design process.\nI enjoy discovering new fields. I’ve collaborated on a wide range of projects, including computer-based assessment (CBA), user-centric cybersecurity, digital art, game design, XR for off-highway machinery, sustainability… Every project offers a new opportunity to explore how design can impact people."
  },
  {
    "objectID": "people/phd_candidates/wang_ziming.html",
    "href": "people/phd_candidates/wang_ziming.html",
    "title": "Ziming Wang",
    "section": "",
    "text": "Home\n    People\n    PhD Candidates\n    Ziming Wang"
  },
  {
    "objectID": "people/phd_candidates/wang_ziming.html#hi",
    "href": "people/phd_candidates/wang_ziming.html#hi",
    "title": "Ziming Wang",
    "section": "Hi!",
    "text": "Hi!\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "people/past_members/defossez_aurelien.html",
    "href": "people/past_members/defossez_aurelien.html",
    "title": "Aurélien Defossez",
    "section": "",
    "text": "Home\n    People\n    Research Staff\n    Aurélien Defossez"
  },
  {
    "objectID": "people/past_members/defossez_aurelien.html#bio",
    "href": "people/past_members/defossez_aurelien.html#bio",
    "title": "Aurélien Defossez",
    "section": "Bio",
    "text": "Bio\nAfter graduating with honours from INSA Lyon — one of the best Science and Technology universities in Europe — Aurélien stepped up his game by working in several innovative and fast-growing tech startups in France. As a ludo-addict and experimentalist, he regularly participates in game jams and constantly designs and creates bite-sized video games, board games and web applications. He’s fascinated by Artificial Intelligence and developed artificial agents as well as two games specifically designed to be played by AI agents (for Cod’INSA, an AI contest). In 2013, he founded an independent game studio (Tabemasu Games) and released a fast-paced mobile game (Kawaii Killer)."
  },
  {
    "objectID": "people/past_members/schmuck_emmanuel.html",
    "href": "people/past_members/schmuck_emmanuel.html",
    "title": "Emmanuel Schmück",
    "section": "",
    "text": "Home\n    People\n    PhD Candidates\n    Emmanuel Schmück"
  },
  {
    "objectID": "people/past_members/schmuck_emmanuel.html#hi",
    "href": "people/past_members/schmuck_emmanuel.html#hi",
    "title": "Emmanuel Schmück",
    "section": "Hi!",
    "text": "Hi!\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "xCIT",
    "section": "",
    "text": "PI, Prof. Cognitive Sciences and Assessment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD, Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD, Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUX Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Candidate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Candidate\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#principal-investigator",
    "href": "people.html#principal-investigator",
    "title": "xCIT Lab",
    "section": "",
    "text": "Cognitive Sciences and Assessment\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "People"
    ]
  },
  {
    "objectID": "people.html#current-members",
    "href": "people.html#current-members",
    "title": "xCIT Lab",
    "section": "Current members",
    "text": "Current members\n\n\n\n\n\n\n\n\n\n\nMorteza Ansarinia\n\n\nResearcher\n\n\n\n\n\n\n\n\n\n\n\n\n\nHoorieh Afkari\n\n\nResearcher\n\n\n\n\n\n\n\n\n\n\n\n\n\nSophie Doublet\n\n\nUX Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\nHainan Yu\n\n\nPhD Candidate\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuvadeep Mukherjee\n\n\nPhD Candidate\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "People"
    ]
  },
  {
    "objectID": "people.html#past-members",
    "href": "people.html#past-members",
    "title": "xCIT Lab",
    "section": "Past members",
    "text": "Past members\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Name\n        \n         \n          Role\n        \n         \n          Started\n        \n         \n          Ended\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nRole\n\n\nStarted\n\n\nEnded\n\n\n\n\n\n\nBrice Clocher\n\n\nGame designer and developer\n\n\nJune 2017\n\n\nJanuary 2021\n\n\n\n\nAurélien Defossez\n\n\nGame designer and developer\n\n\nJune 2017\n\n\nJanuary 2021\n\n\n\n\nDominic Mussac\n\n\nPostdoctoral researcher\n\n\nJuly 2025\n\n\nCurrent\n\n\n\n\nEmmanuel Schmück\n\n\nPhD Candidate\n\n\nJuly 2025\n\n\nCurrent\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "People"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Our research is organized around several key projects that advance our understanding of cognitive science through digital technologies."
  },
  {
    "objectID": "projects.html#current-projects",
    "href": "projects.html#current-projects",
    "title": "Projects",
    "section": "Current Projects",
    "text": "Current Projects\n\nBehaverse\nBehaverse is the main project of the xcit research group. It is a collection of integrated projects and software systems aimed at accelerating, integrating, and scaling up cognitive science research. The initiative fosters scientific discoveries and innovation while promoting best practices and open science.\nThe project is organized into “galaxies,” which are thematic collections of projects. These galaxies encompass a range of development stages, from fully operational systems to prototypes and internal tools. For more details, visit the Behaverse website.\n\n\nCERISE: green skills for sustainability\nCERISE aims to strengthen the capacity of European Higher Education Institutions (HEIs) to deliver courses and services that enhance and recognize green skills while fostering a green culture. The project focuses on two main objectives: supporting the development of higher education curricula to advance students’ green skills and attitudes through digital technologies, and creating a technological tool to facilitate the recognition of green skills in the HE sector.\nThe project is funded by the European Union’s Erasmus+ program and is coordinated by the University of Luxembourg, with partners including Web2Learn, the National Technical University of Athens, Université de Lorraine, the University of Information Technology and Management in Rzeszów, and Kaunas University of Technology. The implementation period is from February 2024 to May 2026. Learn more here.\n\n\nCollaboration Research with video games\nComing soon\n\n\nCognitive sciences and Cybersecurity\nComing soon\n\n\nResearch on Mental Health in Youth\nComing soon"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "We believe in open science and sharing our research tools and data with the scientific community. Here you’ll find software, datasets, and educational materials developed by our lab."
  },
  {
    "objectID": "resources.html#software-tools",
    "href": "resources.html#software-tools",
    "title": "Resources",
    "section": "Software & Tools",
    "text": "Software & Tools\nComing soon"
  },
  {
    "objectID": "resources.html#datasets",
    "href": "resources.html#datasets",
    "title": "Resources",
    "section": "Datasets",
    "text": "Datasets\nComing soon"
  },
  {
    "objectID": "resources.html#other-materials",
    "href": "resources.html#other-materials",
    "title": "Resources",
    "section": "Other Materials",
    "text": "Other Materials\nComing soon"
  },
  {
    "objectID": "resources.html#external-resources",
    "href": "resources.html#external-resources",
    "title": "Resources",
    "section": "External Resources",
    "text": "External Resources\nComing soon"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs & Facilities",
    "section": "",
    "text": "The xCIT research group runs cutting-edge facilities designed to support a wide range of research and development activities. From iterative design and prototyping to large-scale behavioral studies and computational analyses, our labs enable advanced work in cognitive science and digital technology development. The two labs run by the xCIT research group are the UserLab and the BehaverseLab.\n\nThe UserLab\nThe UserLab is part of the Epsylon network of research labs. It is a versatile space designed for user experience (UX) research and supports every stage of the iterative design process—from need analysis and ideation to prototyping and user testing.\nBy combining state-of-the-art technology in a welcoming environment, the UserLab supports a rich set of empirical approaches (e.g., interviews, user testing) and data collection methods (e.g., video recording, eye and movement tracking, physiological recordings). The UserLab therefore allows us to gain a very rich and detailed understanding of a situation, phenomenon, or user experience—albeit on a rather small scale.\n\n\n\nThe Behaverse-Lab\nWhile the UserLab is designed to gain deep insights from a limited number of participants, the xCIT-Lab is equipped for large-scale online studies. It comprises software and hardware to deploy online studies, collect and process large datasets—here benefiting in particular from the University of Luxembourg’s High Performance Computers. The Behaverse-Lab also supports the design and development of technologies, such as video games or software for automated data analyses. Finally, the Behaverse-Lab enables the exploration, prototyping, and experimentation with innovative technologies and hardware (e.g., exploring the potential use of edge devices to collect and process data for research and impact)."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "We teach the following courses at the University of Luxembourg.\n\nComputational Cognitive Sciences\nColloquium: Cognitive Sciences, Education and Assessment (COSEA)\nDesigning Effective Data Visualizations\nIntroduction to Programming in R\nIntroduction to Data Science in R\nStructural Equation Modeling"
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching",
    "section": "Courses",
    "text": "Courses\n\nComputational Cognitive Sciences\nColloquium: Cognitive Sciences, Education and Assessment (COSEA)\nDesigning Effective Data Visualizations\nIntroduction to Programming in R\nIntroduction to Data Science in R\nStructural Equation Modeling",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "teaching.html#workshops",
    "href": "teaching.html#workshops",
    "title": "Teaching",
    "section": "Workshops",
    "text": "Workshops\nComing soon",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "teaching.html#student-research-projects",
    "href": "teaching.html#student-research-projects",
    "title": "Teaching",
    "section": "Student Research Projects",
    "text": "Student Research Projects\nComing soon\nFor immediate inquiries about student projects, please contact us.",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "values.html",
    "href": "values.html",
    "title": "Values & Culture",
    "section": "",
    "text": "Home\n    Values & Culture"
  },
  {
    "objectID": "values.html#our-values",
    "href": "values.html#our-values",
    "title": "Values & Culture",
    "section": "Our Values",
    "text": "Our Values\n\nHumane\nIn our group we want everyone to feel valued, appreciated, safe, empowered to contribute to science and free to be who they are. We embrace diversity and are committed to preventing discrimination against people based on gender identity or expression, sexual orientation, race, ethnicity, religion, age, neurodiversity, disability status, citizenship, or any other feature that makes them unique.\nWe prioritize ethical research practices, GDPR compliance, ethics approval, and accessibility in all our work.\n\n\nOpen Science and Research Integrity\nWe take science very seriously and adhere to its core values of integrity, transparency and sharing. We are committed to open science. Data and source code for our recent projects are available on GitHub. However, data and code for older projects may not be available or formatted in a way that would allow people to use them (e.g., lack of documentation).\n\n\nSustainability\nThe dramatic consequences of human induced climate change are growing at an alarming pace and it is vital for us humans to act for a sustainable future. Everyone can act. The xCIT lab is committed to reduce its negative impacts on the environment (e.g., travel, energy consumption, purchases) and to actively contribute towards a sustainable future."
  },
  {
    "objectID": "values.html#lab-guide",
    "href": "values.html#lab-guide",
    "title": "Values & Culture",
    "section": "Lab Guide",
    "text": "Lab Guide\nWe don’t yet have a comprehensive lab guide outlining xCIT’s principles and practices; we may take inspiration from the Poldrack Lab Guide in the future."
  },
  {
    "objectID": "index.html#what-makes-us-unique",
    "href": "index.html#what-makes-us-unique",
    "title": "Welcome to xCIT!",
    "section": "What makes us unique?",
    "text": "What makes us unique?\nOur work bridges research and application: we design, build, and test scientifically grounded interventions with the goal of making a measurable difference in people’s lives.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "people.html#people",
    "href": "people.html#people",
    "title": "xCIT",
    "section": "",
    "text": "PI, Prof. Cognitive Sciences and Assessment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD, Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD, Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUX Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Candidate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Candidate\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#past-members-incomplete",
    "href": "people.html#past-members-incomplete",
    "title": "xCIT",
    "section": "Past members (incomplete)",
    "text": "Past members (incomplete)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Name\n        \n         \n          Role\n        \n         \n          Started\n        \n         \n          Ended\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nRole\n\n\nStarted\n\n\nEnded\n\n\n\n\n\n\nBrice Clocher\n\n\nGame designer and developer\n\n\nJune 2017\n\n\nJanuary 2021\n\n\n\n\nAurélien Defossez\n\n\nGame designer and developer\n\n\nJune 2017\n\n\nJanuary 2021\n\n\n\n\nKamelia Jamaati\n\n\nPhD Candidate\n\n\nNA\n\n\nNA\n\n\n\n\nDominic Mussac\n\n\nPostdoctoral researcher\n\n\nNA\n\n\nNA\n\n\n\n\nEmmanuel Schmück\n\n\nPhD Candidate\n\n\nNA\n\n\nNA\n\n\n\n\nZiming Wang\n\n\nPhD Candidate\n\n\nNA\n\n\nNovember 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "git_ignored/TODO.html",
    "href": "git_ignored/TODO.html",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Updated navigation menu to follow Poldrack Lab structure\nAdded “About” dropdown menu with Values, Labs, and Collaborations\nReorganized navigation: Research → Projects → Publications → People → Teaching → Resources → About → Join Us → Contact\nEnhanced home page with section overview and clear calls-to-action\nAdded breadcrumb navigation to all main pages\nImproved page formatting and structure for all main sections\n\n\n\n\n\n\n\nProjects page: Add detailed descriptions for Behaverse and CERISE projects\nPublications page: Implement automated Google Scholar integration\nTeaching page: Add detailed course descriptions and learning objectives\nResources page: Add actual links to GitHub repositories and datasets\nCollaborations page: Add affiliations and collaboration details for all partners\n\n\n\n\n\nFix empty href in line 35: “Learn more about the PhD journey at the University of Luxembourg”\nFix empty href in line 45: “Learn more about this position” (PhD in AI and Cognitive Sciences)\nFix empty href in line 56: “Learn more about this position” (PhD in Cybersecurity)\nFix empty href in line 94: “Learn more about Jobs available at UniLu”\n\n\n\n\nIn research.qmd: - [ ] Line 25: Fix “cognitve assessment” → “cognitive assessment” - [ ] Line 25: Fix “cognitve training” → “cognitive training”\nIn people/pi/cardosoleite_pedro.qmd: - [ ] Line 46: Fix “pyschologist” → “psychologist” - [ ] Line 48: Fix “impac” → “impact” - [ ] Line 48: Fix “suporting” → “supporting” - [ ] Line 52: Fix “partiuclar” → “particular” - [ ] Line 52: Fix “Univerisy” → “University” - [ ] Line 52: Fix “reasearch” → “research”\nIn join_us.qmd: - [ ] Line 69: Fix “Univeristy” → “University”\n\n\n\nIn research.qmd: - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive assessment - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive training - [ ] Line 39: Verify and fix “behaverse.org” link\n\n\n\n\n\n\nReplace placeholder.png with actual photos for: - [ ] Hoorieh Afkari (people/members/afkari_hoorieh.qmd) - [ ] Sophie Doublet (people/members/doublet_sophie.qmd) - [ ] Suvadeep Mukherjee (people/members/mukherjee_suvadeep.qmd) - [ ] Hainan Yu (people/members/yu_hainan.qmd) - [ ] Ziming Wang (people/phd_candidates/wang_ziming.qmd) - [ ] Dominic Mussac (people/past_members/mussac_dominic.qmd) - [ ] Emmanuel Schmuck (people/past_members/schmuck_emmanuel.qmd)\n\n\n\n\nHome page: Add lab photo or showcase images\nResearch page: Complete fragmented sentences and improve flow\nProjects page: Add project timelines, funding information, and outcomes\nTeaching page: Add course schedules, prerequisites, and enrollment information\nValues page: Consider adding a formal lab manual/guide\nLabs page: Add photos of facilities and equipment\n\n\n\n\n\nCheck and update all team member status and roles for 2025\nStandardize bio formats across all team members\nVerify all external links in team member profiles work correctly\nAdd research interests and key publications to each profile\n\n\n\n\n\n\n\n\nConsider implementing design elements inspired by Anthropic/IDEO websites\nAdd visual hierarchy improvements\nImplement better grid layouts for content sections\nConsider custom CSS for better visual appeal\nAdd hover effects and interactive elements\n\n\n\n\n\nAdd meta descriptions for better SEO\nAdd keywords for search optimization\nReview color contrast for accessibility compliance\nAdd alt text for all images\nOptimize image sizes and formats for better performance\n\n\n\n\n\nAdd search functionality\nConsider adding a news/blog section for lab updates\nAdd contact form instead of just email link\nImplement smooth scrolling and page transitions\nAdd “back to top” buttons on long pages\n\n\n\n\n\nGet professional headshots for all team members\nAdd testimonials or impact stories\nCreate infographics for research methodology\nAdd lab achievements and awards section\nConsider adding timeline of lab milestones\n\n\n\n\n\n\n\n\nImplement automated publication fetching from Google Scholar\nAdd interactive research visualizations\nCreate online lab tour or virtual reality experience\nAdd multilingual support (English/French/German)\nImplement content management system for easier updates\n\n\n\n\n\nSocial media integration\nGoogle Analytics tracking verification\nNewsletter signup integration\nEvent calendar integration\nOnline application system for positions\n\n\n\n\n\n\nTest all external links\nVerify all internal navigation works correctly\nCheck website responsiveness on mobile devices\nValidate HTML markup\nTest website loading speed\nEnsure all images load correctly\nVerify contact email works correctly\nTest new navigation structure across all devices\n\n\nNote: The website structure has been significantly improved to match the Poldrack Lab organization. Priority should now focus on content development and fixing remaining technical issues.\nLast Updated: September 3, 2025"
  },
  {
    "objectID": "git_ignored/TODO.html#completed-website-structure-reorganization",
    "href": "git_ignored/TODO.html#completed-website-structure-reorganization",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Updated navigation menu to follow Poldrack Lab structure\nAdded “About” dropdown menu with Values, Labs, and Collaborations\nReorganized navigation: Research → Projects → Publications → People → Teaching → Resources → About → Join Us → Contact\nEnhanced home page with section overview and clear calls-to-action\nAdded breadcrumb navigation to all main pages\nImproved page formatting and structure for all main sections"
  },
  {
    "objectID": "git_ignored/TODO.html#high-priority-issues-critical-fixes",
    "href": "git_ignored/TODO.html#high-priority-issues-critical-fixes",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Projects page: Add detailed descriptions for Behaverse and CERISE projects\nPublications page: Implement automated Google Scholar integration\nTeaching page: Add detailed course descriptions and learning objectives\nResources page: Add actual links to GitHub repositories and datasets\nCollaborations page: Add affiliations and collaboration details for all partners\n\n\n\n\n\nFix empty href in line 35: “Learn more about the PhD journey at the University of Luxembourg”\nFix empty href in line 45: “Learn more about this position” (PhD in AI and Cognitive Sciences)\nFix empty href in line 56: “Learn more about this position” (PhD in Cybersecurity)\nFix empty href in line 94: “Learn more about Jobs available at UniLu”\n\n\n\n\nIn research.qmd: - [ ] Line 25: Fix “cognitve assessment” → “cognitive assessment” - [ ] Line 25: Fix “cognitve training” → “cognitive training”\nIn people/pi/cardosoleite_pedro.qmd: - [ ] Line 46: Fix “pyschologist” → “psychologist” - [ ] Line 48: Fix “impac” → “impact” - [ ] Line 48: Fix “suporting” → “supporting” - [ ] Line 52: Fix “partiuclar” → “particular” - [ ] Line 52: Fix “Univerisy” → “University” - [ ] Line 52: Fix “reasearch” → “research”\nIn join_us.qmd: - [ ] Line 69: Fix “Univeristy” → “University”\n\n\n\nIn research.qmd: - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive assessment - [ ] Line 25: Replace “(REF)” placeholders with actual citations or links for cognitive training - [ ] Line 39: Verify and fix “behaverse.org” link"
  },
  {
    "objectID": "git_ignored/TODO.html#medium-priority-issues",
    "href": "git_ignored/TODO.html#medium-priority-issues",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Replace placeholder.png with actual photos for: - [ ] Hoorieh Afkari (people/members/afkari_hoorieh.qmd) - [ ] Sophie Doublet (people/members/doublet_sophie.qmd) - [ ] Suvadeep Mukherjee (people/members/mukherjee_suvadeep.qmd) - [ ] Hainan Yu (people/members/yu_hainan.qmd) - [ ] Ziming Wang (people/phd_candidates/wang_ziming.qmd) - [ ] Dominic Mussac (people/past_members/mussac_dominic.qmd) - [ ] Emmanuel Schmuck (people/past_members/schmuck_emmanuel.qmd)\n\n\n\n\nHome page: Add lab photo or showcase images\nResearch page: Complete fragmented sentences and improve flow\nProjects page: Add project timelines, funding information, and outcomes\nTeaching page: Add course schedules, prerequisites, and enrollment information\nValues page: Consider adding a formal lab manual/guide\nLabs page: Add photos of facilities and equipment\n\n\n\n\n\nCheck and update all team member status and roles for 2025\nStandardize bio formats across all team members\nVerify all external links in team member profiles work correctly\nAdd research interests and key publications to each profile"
  },
  {
    "objectID": "git_ignored/TODO.html#low-priority-improvements",
    "href": "git_ignored/TODO.html#low-priority-improvements",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Consider implementing design elements inspired by Anthropic/IDEO websites\nAdd visual hierarchy improvements\nImplement better grid layouts for content sections\nConsider custom CSS for better visual appeal\nAdd hover effects and interactive elements\n\n\n\n\n\nAdd meta descriptions for better SEO\nAdd keywords for search optimization\nReview color contrast for accessibility compliance\nAdd alt text for all images\nOptimize image sizes and formats for better performance\n\n\n\n\n\nAdd search functionality\nConsider adding a news/blog section for lab updates\nAdd contact form instead of just email link\nImplement smooth scrolling and page transitions\nAdd “back to top” buttons on long pages\n\n\n\n\n\nGet professional headshots for all team members\nAdd testimonials or impact stories\nCreate infographics for research methodology\nAdd lab achievements and awards section\nConsider adding timeline of lab milestones"
  },
  {
    "objectID": "git_ignored/TODO.html#future-enhancements",
    "href": "git_ignored/TODO.html#future-enhancements",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Implement automated publication fetching from Google Scholar\nAdd interactive research visualizations\nCreate online lab tour or virtual reality experience\nAdd multilingual support (English/French/German)\nImplement content management system for easier updates\n\n\n\n\n\nSocial media integration\nGoogle Analytics tracking verification\nNewsletter signup integration\nEvent calendar integration\nOnline application system for positions"
  },
  {
    "objectID": "git_ignored/TODO.html#verification-tasks",
    "href": "git_ignored/TODO.html#verification-tasks",
    "title": "xCIT Website TODO List",
    "section": "",
    "text": "Test all external links\nVerify all internal navigation works correctly\nCheck website responsiveness on mobile devices\nValidate HTML markup\nTest website loading speed\nEnsure all images load correctly\nVerify contact email works correctly\nTest new navigation structure across all devices\n\n\nNote: The website structure has been significantly improved to match the Poldrack Lab organization. Priority should now focus on content development and fixing remaining technical issues.\nLast Updated: September 3, 2025"
  },
  {
    "objectID": "git_ignored/collaborations.html",
    "href": "git_ignored/collaborations.html",
    "title": "Collaborations",
    "section": "",
    "text": "We enjoy collaborating with experts from a diversity of backgrounds and institutions. Our collaborative approach enriches our research and extends our impact."
  },
  {
    "objectID": "git_ignored/collaborations.html#long-standing-collaborators",
    "href": "git_ignored/collaborations.html#long-standing-collaborators",
    "title": "Collaborations",
    "section": "Long-Standing Collaborators",
    "text": "Long-Standing Collaborators\n\nAcademic Partners\n\nDaphne Bavelier - [Add affiliation and collaboration focus]\nPaul Schrater - [Add affiliation and collaboration focus]\nFlorian Waszak - [Add affiliation and collaboration focus]\nEmmanuel Schmuck - [Add affiliation and collaboration focus]"
  },
  {
    "objectID": "git_ignored/collaborations.html#institutional-partnerships",
    "href": "git_ignored/collaborations.html#institutional-partnerships",
    "title": "Collaborations",
    "section": "Institutional Partnerships",
    "text": "Institutional Partnerships\n\nUniversity of Luxembourg\n[Add information about internal collaborations]\n\n\nInternational Collaborations\n[Add information about international research partnerships]"
  },
  {
    "objectID": "git_ignored/collaborations.html#industry-partnerships",
    "href": "git_ignored/collaborations.html#industry-partnerships",
    "title": "Collaborations",
    "section": "Industry Partnerships",
    "text": "Industry Partnerships\n[Add information about industry collaborations and technology transfer]"
  },
  {
    "objectID": "git_ignored/collaborations.html#collaborative-projects",
    "href": "git_ignored/collaborations.html#collaborative-projects",
    "title": "Collaborations",
    "section": "Collaborative Projects",
    "text": "Collaborative Projects\nFor specific research projects involving collaborations, see our Projects page."
  },
  {
    "objectID": "git_ignored/collaborations.html#interested-in-collaborating",
    "href": "git_ignored/collaborations.html#interested-in-collaborating",
    "title": "Collaborations",
    "section": "Interested in Collaborating?",
    "text": "Interested in Collaborating?\nWe’re always open to new collaborative opportunities. If you’re interested in working with us, please get in touch with a brief description of your research interests and potential collaboration ideas."
  },
  {
    "objectID": "git_ignored/contact.html",
    "href": "git_ignored/contact.html",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us!\nFeel free to get in touch with us at contact@xcit.org\n\nThe xCIT Lab is located in the Belval Campus of the University of Luxembourg: you can find us in the “Maison des Sciences Humaines”."
  },
  {
    "objectID": "git_ignored/contact.html#get-in-touch",
    "href": "git_ignored/contact.html#get-in-touch",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us!\nFeel free to get in touch with us at contact@xcit.org\n\nThe xCIT Lab is located in the Belval Campus of the University of Luxembourg: you can find us in the “Maison des Sciences Humaines”."
  },
  {
    "objectID": "git_ignored/join_us.html",
    "href": "git_ignored/join_us.html",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us! The currently open positions are listed below. It might also be possible for us to offer opportunities for internships in data science (analysis of large scale behavioral data, computational modeling), game design and development, computer engineering (web apps), and digital arts for video games (graphics, animations, sound).\nFeel free to get in touch with us at contact@xcit.org to find out more.\n\n\n\nThere are currently 2 fully-funded PhD candidate positions open in the xCIT lab.\n\n  Learn more about the PhD journey at the University of Luxembourg\n\n\n\n\n  Learn more about this position\n\n\n\n\nThis is a position funded by the SnT with whom we are collaborating with on a project that aims to assess teams’s readiness level to handle various forms of cyber attacks.\n\n  Learn more about this position"
  },
  {
    "objectID": "git_ignored/join_us.html#join-us",
    "href": "git_ignored/join_us.html#join-us",
    "title": "xCIT Lab",
    "section": "",
    "text": "We are always happy to hear from people interested in joining or collaborating with us! The currently open positions are listed below. It might also be possible for us to offer opportunities for internships in data science (analysis of large scale behavioral data, computational modeling), game design and development, computer engineering (web apps), and digital arts for video games (graphics, animations, sound).\nFeel free to get in touch with us at contact@xcit.org to find out more.\n\n\n\nThere are currently 2 fully-funded PhD candidate positions open in the xCIT lab.\n\n  Learn more about the PhD journey at the University of Luxembourg\n\n\n\n\n  Learn more about this position\n\n\n\n\nThis is a position funded by the SnT with whom we are collaborating with on a project that aims to assess teams’s readiness level to handle various forms of cyber attacks.\n\n  Learn more about this position"
  },
  {
    "objectID": "git_ignored/join_us.html#master-and-bachelor-students-projects",
    "href": "git_ignored/join_us.html#master-and-bachelor-students-projects",
    "title": "xCIT Lab",
    "section": "Master and Bachelor students projects",
    "text": "Master and Bachelor students projects\nThere are several ways to join the xCIT research group as a Master or Bachelor student. In each case you should contact us and send us your CV as well as samples of your past work (e.g., code on GitHub, Bachelor thesis).\n\nStudent assistant job\nThis is a paid position, applicable only for students at the Univeristy of Luxembourg, and corresponding to a maximum workload of 40H/week.\nWe typically only accept one to two student assistants per year. If you have good programming skills (e.g., in Python) and are autonomous and well-organized, your chances are better.\n\n\nInternships\nWe can support students who want to complete an internship in our lab. Internships can last somewhere between 2 and 6 months.\n\n\nBachelor or Master thesis\nIf you are enrolled at the University of Luxembourg and would like to do your Bachelor or Master thesis research work with us, we might be able to suggest research topics and support you in completing that research work."
  },
  {
    "objectID": "git_ignored/join_us.html#citizen-science",
    "href": "git_ignored/join_us.html#citizen-science",
    "title": "xCIT Lab",
    "section": "Citizen Science",
    "text": "Citizen Science\nIf you have technical, artistic or other skills that you would like to put to use for science, feel free to reach out; there is always more work and ideas than hands to build stuff;-)."
  },
  {
    "objectID": "git_ignored/join_us.html#other-jobs",
    "href": "git_ignored/join_us.html#other-jobs",
    "title": "xCIT Lab",
    "section": "Other jobs",
    "text": "Other jobs\nThe University of Luxembourg is a great place for researchers, and there are often great job opportunities. You can learn about open positions at the University of Luxembourg.\nLearn more about Jobs available at UniLu"
  },
  {
    "objectID": "git_ignored/research.html",
    "href": "git_ignored/research.html",
    "title": "xCIT Lab",
    "section": "",
    "text": "Home\n    Research"
  },
  {
    "objectID": "git_ignored/research.html#research-topics",
    "href": "git_ignored/research.html#research-topics",
    "title": "xCIT Lab",
    "section": "Research Topics",
    "text": "Research Topics\n\nCognitive sciences\nThe main research focus of xCIT is cognitive science: which includes psychology, cognitive neurosciences and computer sciences (AI).\nWe use the methods, theories and tools from these fields to gain insights about the human mind and brain, and then to use those insights to possibly create new tools and theories.\nWe are currently actively working on cognitve assessment (REF) and cognitive training (REF). We are also working on\nusing insights and methods across fields like experimental psychology, neuroscience and AI to study the human mind.\nWe are working on cognitive training with video games Studying collaboration skills\n\n\nScientific methods, tools and practices\nWe invest a large chunk of our time in developing methods, tools and practices to improve how we do cognitive science research. standards, api, etc\nYou can learn more about this aspect of our work on behaverse.org\n\n\nDesign and Development\nUX software dev prototyping game design and dev (in collaboration with external partners)\n\n\nApplication domains for societal impact\nCognitive training, Human learning, Mental health, Sustainability Cybersecurity"
  },
  {
    "objectID": "git_ignored/research.html#research-methods",
    "href": "git_ignored/research.html#research-methods",
    "title": "xCIT Lab",
    "section": "Research Methods",
    "text": "Research Methods\nWe use a variety of methods; in general, we aim to use the best tools for the situation at hand rather than to find problems that can be addressed with the tools we already have.\n\nExperimental psychology\nThe main research approach involves conducting behavioral experiments where we collect data from participants engaged in various activities.\n\n\nVideo games design and development\n\n\nComputational methods and Applied Machine Learning\n\n\nUX design"
  },
  {
    "objectID": "people/past_members/wang_ziming.html",
    "href": "people/past_members/wang_ziming.html",
    "title": "Ziming Wang",
    "section": "",
    "text": "Home\n    People\n    PhD Candidates\n    Ziming Wang"
  },
  {
    "objectID": "people/past_members/wang_ziming.html#hi",
    "href": "people/past_members/wang_ziming.html#hi",
    "title": "Ziming Wang",
    "section": "Hi!",
    "text": "Hi!\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "people/past_members/jamaati_kamelia.html",
    "href": "people/past_members/jamaati_kamelia.html",
    "title": "Kamelia Jamaati",
    "section": "",
    "text": "Home\n    People\n    PhD Candidates\n    Kamelia Jamaati"
  },
  {
    "objectID": "people/past_members/jamaati_kamelia.html#hi",
    "href": "people/past_members/jamaati_kamelia.html#hi",
    "title": "Kamelia Jamaati",
    "section": "Hi!",
    "text": "Hi!\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "git_ignored/values.html",
    "href": "git_ignored/values.html",
    "title": "Values & Culture",
    "section": "",
    "text": "Home\n    Values & Culture"
  },
  {
    "objectID": "git_ignored/values.html#our-values",
    "href": "git_ignored/values.html#our-values",
    "title": "Values & Culture",
    "section": "Our Values",
    "text": "Our Values\n\nHumane\nIn our group we want everyone to feel valued, appreciated, safe, empowered to contribute to science and free to be who they are. We embrace diversity and are committed to preventing discrimination against people based on gender identity or expression, sexual orientation, race, ethnicity, religion, age, neurodiversity, disability status, citizenship, or any other feature that makes them unique.\nWe prioritize ethical research practices, GDPR compliance, ethics approval, and accessibility in all our work.\n\n\nOpen Science and Research Integrity\nWe take science very seriously and adhere to its core values of integrity, transparency and sharing. We are committed to open science. Data and source code for our recent projects are available on GitHub. However, data and code for older projects may not be available or formatted in a way that would allow people to use them (e.g., lack of documentation).\n\n\nSustainability\nThe dramatic consequences of human induced climate change are growing at an alarming pace and it is vital for us humans to act for a sustainable future. Everyone can act. The xCIT lab is committed to reduce its negative impacts on the environment (e.g., travel, energy consumption, purchases) and to actively contribute towards a sustainable future."
  },
  {
    "objectID": "git_ignored/values.html#lab-guide",
    "href": "git_ignored/values.html#lab-guide",
    "title": "Values & Culture",
    "section": "Lab Guide",
    "text": "Lab Guide\nWe don’t yet have a comprehensive lab guide outlining xCIT’s principles and practices; we may take inspiration from the Poldrack Lab Guide in the future."
  }
]